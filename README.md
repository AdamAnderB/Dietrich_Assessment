# ðŸ§  Building Better Assessment: Scalable Tools for Human and AI Scoring

## ðŸ“š *Reference:*
**Bramlett, A. A.** (2025). *From Bottlenecks to Breakthroughs: Building Secure, Scalable, and Data-Driven Assessment Systems.* Graduate Assessment Fellowship Report, Dietrich College, CMU.

---

## ðŸ“Œ Overview

This project provides a scalable, transparent infrastructure for educational assessment, developed during the Graduate Assessment Fellowship at Carnegie Mellon University. It features a secure scoring app, a robust reliability framework, and an AI-assisted workflow for evaluating student artifacts. All tools are designed to support both human and AI raters, with attention to reproducibility, FERPA compliance, and modular design.

---

## ðŸ“‚ Repository Contents

### ðŸ§© `survey revised/app.R`

**Dynamic Scoring App (R + Shiny)**  
A secure, user-friendly interface for scoring student work with embedded prompts, rubrics, and real-time validation. Built to streamline rater workflow and ensure scoring accuracy.

---

**Before (left) and after (right): manual vs. app-based rating workflow**

<img src="figures/rating_app.png" alt="Shiny App Interface" width="450">

---

**Example of the interactive scoring interface**

<img src="figures/rubric_scoring.png" alt="Rubric Scoring Interface" width="450">

---

### ðŸ“ˆ `reliability.Rmd`

**Reliability Analysis Script (R Markdown)**  
Calculates inter-rater agreement using exact match, one-off agreement, and Cohenâ€™s Kappa. Built to compare human-human and AI-human ratings, with visualization support.

---

**Humanâ€“Human reliability over time**

<img src="figures/overal_reliability.png" alt="Rater Agreement Over Time" width="400">

---

### ðŸ§  `survey revised/work_flow_updated.Rmd`

**AI-Assisted Artifact Scoring Pipeline**  
An LLM-powered system using GPT-4.0 mini (via CMUâ€™s secure LiteLLM deployment) for rubric-aligned artifact evaluation. Includes prompt generation, response tracking, and audit-ready output.

---

**LLMâ€“LLM and LLMâ€“Human scoring comparison**

<img src="figures/AI vs AI and Human vs AI Agreement by Rubric Component.png" alt="AI vs Human Agreement" width="500">

---

## ðŸŒŸ Key Features

- ðŸ” **FERPA-Compliant Infrastructure**  
  All scoring processes are secure and internal to CMU systems.

- âš–ï¸ **Human & AI Calibration**  
  Multi-level reliability checks and detailed rubric diagnostics.

- ðŸ¤– **LLM Integration**  
  GPT-4.0 mini deployed as a secure, scalable rater within the assessment system.

- ðŸ“Š **Tidy, Structured Output**  
  Data formatted for immediate aggregation, analysis, and visualization in R.

---

## ðŸ› ï¸ Getting Started

1. Clone this repository.
2. Launch the Shiny app via `app.R` for human scoring.
3. Run `reliability.Rmd` to evaluate consistency across raters or compare AI to human scores.
4. Use `work_flow_updated.Rmd` to score artifacts using GPT-4.0 mini via CMUâ€™s LiteLLM proxy.

---

## ðŸ“£ Citation

Please cite the original report if this toolkit assists your research or program development:

> Bramlett, A. A. (2025). *From Bottlenecks to Breakthroughs: Building Secure, Scalable, and Data-Driven Assessment Systems.* Graduate Assessment Fellowship Report, Dietrich College, CMU.
